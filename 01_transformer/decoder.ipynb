{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e79f7b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math \n",
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e38cfe2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(size=(3,3))\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8edbb6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" \n",
    "Multi-Head-Attention + (Self attention):\n",
    "\"\"\"\n",
    "def selfAttention(q,k,v,mask=None): # 30 x 8 x 200 x 64\n",
    "    d_k = k.size()[-1]\n",
    "    scaled = torch.matmul(q,k.transpose(-1,-2)) / math.sqrt(d_k) # 30 x 8 x 200 x 200\n",
    "    if mask is not None:\n",
    "        scaled =+ mask  \n",
    "    attention = F.softmax(input=scaled,dim=-1)  # 30 x 8 x 200 x 200 \n",
    "    # (30 x 8 x 200 x 200) (30 x 8 x 200 x 64) =  (30 x 8 x 200 x 64) \n",
    "    values = torch.matmul(attention,v)  #  30 x 8 x 200 x 64\n",
    "    return attention,values\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # 512\n",
    "        self.num_heads = num_heads # 8\n",
    "        self.head_dim = d_model // num_heads # 512/8 = 64\n",
    "        self.qkv = nn.Linear(in_features=d_model,out_features=(3*d_model))\n",
    "        self.linear_trans = nn.Linear(in_features=d_model,out_features=d_model)\n",
    "        \n",
    "    def forward(self,x,mask):\n",
    "        batch_size,max_sequence_length,d_model = x.size() # 30 x 200 x 512 \n",
    "        qkv = self.qkv(x) # 30 x 200 x 1536\n",
    "        qkv = qkv.view(batch_size,max_sequence_length,self.num_heads, 3 * self.head_dim) # 30 x 200 x 8 x (3*64)=192\n",
    "        qkv = qkv.permute(0,2,1,3)  # 30 x 8 X 200 x 192 \n",
    "        q,k,v = qkv.chunk(3,dim=-1) # 30 x 8 x 200 x 64\n",
    "        # attention=( 30 x 8 x 200 x 200) vlaues=( 30 x 8 x 200 x 64)\n",
    "        attention, values = selfAttention(q=q,k=k,v=v,mask=mask) \n",
    "        # [30, 8, 200, 64] -> [30,200,8,64] -> [30,200,512]\n",
    "        values = values.permute(0,2,1,3).reshape(batch_size,max_sequence_length,self.num_heads*self.head_dim)\n",
    "        out = self.linear_trans(values)\n",
    "        return out \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05967a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" \n",
    "Cross Attention:\n",
    "query -> From Decoder\n",
    "\n",
    "key ->  From Encoder\n",
    "value -> From Encoder\n",
    "While traning, we need no mask\n",
    "\"\"\"\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self,d_model,num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # 512\n",
    "        self.num_heads = num_heads #8\n",
    "        self.head_dim = d_model // num_heads # 64 \n",
    "        self.kv = nn.Linear(in_features=d_model,out_features=2*d_model)\n",
    "        self.q = nn.Linear(in_features=d_model,out_features=d_model)\n",
    "        self.linear_trans = nn.Linear(in_features=d_model,out_features=d_model)\n",
    "        \n",
    "    def forward(self,x,y,mask):\n",
    "        batch_size,max_sequence_length,d_model = x.size()\n",
    "        kv = self.kv(x) # 30 x 200 x 1024\n",
    "        q = self.q(y) # 30 x 200 x 512\n",
    "        kv = kv.view(batch_size,max_sequence_length,self.num_heads,2*self.head_dim) # 30x200x8x128\n",
    "        q = q.view(batch_size,max_sequence_length,self.num_heads,self.head_dim) # 30x200x8x64\n",
    "        kv = kv.permute(0,2,1,3) # 30x8x200x128\n",
    "        q = q.permute(0,2,1,3) # 30x8x200x64\n",
    "        k,v = kv.chunk(2,dim=-1) # 30 x 8 x 200 x 64 \n",
    "        # attention=( 30 x 8 x 200 x 200) vlaues=( 30 x 8 x 200 x 64)\n",
    "        attention,values = selfAttention(q=q,k=k,v=v,mask=None)\n",
    "        values = values.permute(0,2,1,3).reshape(batch_size,max_sequence_length,self.num_heads*self.head_dim)\n",
    "        out = self.linear_trans(values)\n",
    "        return out \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f1b3e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" \n",
    "LayerNormalization:\n",
    "\"\"\"\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self,parameter_shape,esp=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameter_shape = parameter_shape # 512\n",
    "        self.gamma = nn.Parameter(torch.ones(size=parameter_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(size=parameter_shape))\n",
    "        self.esp = esp \n",
    "    def forward(self,x):\n",
    "        dims = [ -(i+1) for i in range(len(self.parameter_shape))]\n",
    "        mean = x.mean(dim=dims,keepdim=True)\n",
    "        var = ((x-mean) **2).mean(dim=dims,keepdim=True)\n",
    "        std = (var+self.esp).sqrt()\n",
    "        y = (x-mean) / std \n",
    "        out = self.gamma * y + self.beta \n",
    "        return out \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b03abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Feed Forward Network\n",
    "\"\"\"\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,d_model,hidden,drop_prob):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_features=d_model,out_features=hidden)\n",
    "        self.linear2 = nn.Linear(in_features=hidden,out_features=d_model)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropOut = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        x = self.linear1(input)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropOut(x)\n",
    "        x = self.linear2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9f0dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" \n",
    "DecoderLayer\n",
    "\"\"\"\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,num_heads,drop_prob,ffn_hidden):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model,num_heads=num_heads)\n",
    "        self.dropOut = nn.Dropout(p=drop_prob)\n",
    "        self.norm = LayerNormalization(parameter_shape=[d_model])\n",
    "        self.cross = CrossAttention(d_model=d_model,num_heads=num_heads)\n",
    "        self.ffn = FeedForward(d_model=d_model,hidden=ffn_hidden,drop_prob=drop_prob)\n",
    "        \n",
    "    def forward(self,x,y,decoder_mask):\n",
    "        \n",
    "        residual = y # 30x20x512\n",
    "        # mased multi-head attention\n",
    "        y = self.attention(y,decoder_mask)\n",
    "        y = self.dropOut(y)\n",
    "        # add and norm\n",
    "        y = self.norm(residual + y)\n",
    "        \n",
    "        residual = y \n",
    "        y = self.cross(x,y,mask=None)\n",
    "        y = self.dropOut(y)\n",
    "        y = self.norm(residual + y )\n",
    "        \n",
    "        residul = y \n",
    "        y = self.ffn(y)\n",
    "        y = self.dropOut(y)\n",
    "        y = self.norm(residual + y)\n",
    "        return y  # 30 x 200 x 12\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa2115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Final Decoder Layer:\n",
    "\"\"\"\n",
    "class SequentialDecoder(nn.Sequential):\n",
    "    def forward(self,*inputs):\n",
    "        x,y,mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            y = module(x,y,mask)\n",
    "        return y\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,d_model,num_heads,drop_prob,ffn_hidden,num_layer):\n",
    "        super().__init__()\n",
    "        self.layers = SequentialDecoder(*[DecoderLayer(d_model,num_heads,drop_prob,ffn_hidden)\n",
    "                                          for _ in range(num_layer)])\n",
    "    def forward(self,x,y,mask):\n",
    "        y = self.layers(x,y,mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd11e926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask :  tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
      "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
      "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# hyperparameter think from the decoder architecture:\n",
    "d_model = 512\n",
    "drop_prob = 0.1\n",
    "num_heads = 8\n",
    "num_layer = 6\n",
    "batch_size = 30\n",
    "ffn_hidden = 2048\n",
    "max_sequence_length = 200\n",
    "\n",
    "# inputs: \n",
    "x = torch.randn(size=(batch_size,max_sequence_length,d_model)) # English \n",
    "y = torch.randn(size=(batch_size,max_sequence_length,d_model)) # Bangla\n",
    "\n",
    "# mask:\n",
    "mask = torch.tril(input=torch.ones(size=(200,200)))\n",
    "mask[mask==0] = -np.inf\n",
    "mask[mask==1] = 0\n",
    "print(\"mask : \",mask,flush=True)\n",
    "\n",
    "decoder = Decoder(d_model=d_model,num_heads=num_heads,drop_prob=drop_prob,\n",
    "                  ffn_hidden=ffn_hidden,num_layer=num_layer)\n",
    "out = decoder(x, y, mask)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
